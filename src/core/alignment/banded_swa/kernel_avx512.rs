//! AVX-512 specialized fast path for banded SWA (int8 lanes).
//!
//! For now, this forwards to the generic workspace-powered kernel while we
//! land mask-optimized inner loops behind this fa√ßade. Keeping the entry
//! separate allows us to iterate on AVX-512 specific improvements without
//! touching other ISAs.

use super::kernel::{KernelParams, SwSimd, sw_kernel_with_ws};
use crate::core::alignment::banded_swa::OutScore;
use crate::core::alignment::shared_types::WorkspaceArena;

/// AVX-512 fast path wrapper. Currently delegates to the shared kernel.
#[inline]
#[allow(unsafe_op_in_unsafe_fn)]
pub unsafe fn sw_kernel_avx512_with_ws<const W: usize, E: SwSimd>(
    params: &KernelParams<'_>,
    ws: &mut dyn WorkspaceArena,
) -> Vec<OutScore>
where
    <E as SwSimd>::V8: std::fmt::Debug,
{
    // If AVX-512 optimized implementation is available and width matches, use it.
    #[cfg(all(target_arch = "x86_64", feature = "avx512"))]
    {
        if W == 64 {
            return unsafe { sw_kernel_avx512_impl::<W>(params, ws) };
        }
    }

    // Fallback: delegate to the shared kernel.
    sw_kernel_with_ws::<W, E>(params, ws)
}

#[cfg(all(target_arch = "x86_64", feature = "avx512"))]
#[inline]
#[allow(unsafe_op_in_unsafe_fn)]
unsafe fn sw_kernel_avx512_impl<const W: usize>(
    params: &KernelParams<'_>,
    ws: &mut dyn WorkspaceArena,
) -> Vec<OutScore> {
    debug_assert_eq!(W, 64, "AVX-512 fast path expects 64 lanes for i8");

    use crate::core::compute::simd_abstraction::types::simd_arch as avx;

    let stride = W;
    let lanes = params.batch.len().min(params.qlen.len()).min(W);
    let qmax = params.max_qlen.max(0) as usize;
    let tmax = params.max_tlen.max(0) as usize;
    if qmax == 0 || tmax == 0 || lanes == 0 {
        return Vec::new();
    }

    // Extract config
    let (o_del, e_del, o_ins, e_ins, zdrop, mat, _m) = if let Some(cfg) = params.cfg {
        (
            cfg.gaps.o_del,
            cfg.gaps.e_del,
            cfg.gaps.o_ins,
            cfg.gaps.e_ins,
            cfg.banding.zdrop,
            cfg.scoring.mat5x5,
            cfg.scoring.m,
        )
    } else {
        (
            params.o_del,
            params.e_del,
            params.o_ins,
            params.e_ins,
            params.zdrop,
            params.mat,
            params.m,
        )
    };

    // Constants
    let match_score = mat[0];
    let mismatch_score = mat[1];

    // Workspace rows (aligned)
    ws.ensure_rows(stride, qmax, tmax, core::mem::size_of::<i8>());
    let (h_rows, e_rows, _f_rows) = ws
        .rows_u8()
        .expect("WorkspaceArena did not provide u8 rows after ensure_rows");
    debug_assert_eq!(h_rows.len(), qmax * stride);
    debug_assert_eq!(e_rows.len(), qmax * stride);
    let h_ptr = h_rows.as_ptr() as *mut i8;
    let e_ptr = e_rows.as_ptr() as *mut i8;

    // Vectors
    let zero = avx::_mm512_setzero_si512();
    let match_vec = avx::_mm512_set1_epi8(match_score);
    let mismatch_vec = avx::_mm512_set1_epi8(mismatch_score);
    let one = avx::_mm512_set1_epi8(1);

    let oe_del_vec = avx::_mm512_set1_epi8((o_del + e_del) as i8);
    let oe_ins_vec = avx::_mm512_set1_epi8((o_ins + e_ins) as i8);
    let e_del_vec = avx::_mm512_set1_epi8(e_del as i8);
    let e_ins_vec = avx::_mm512_set1_epi8(e_ins as i8);

    // Initialize first row H from h0
    let h0_vec = avx::_mm512_loadu_si512(params.h0.as_ptr() as *const _);
    avx::_mm512_storeu_si512(h_ptr as *mut _, h0_vec);
    let mut h_prev = avx::_mm512_max_epi8(avx::_mm512_subs_epi8(h0_vec, oe_ins_vec), zero);
    avx::_mm512_storeu_si512(h_ptr.add(stride) as *mut _, h_prev);
    for j in 2..qmax {
        let h_curr = avx::_mm512_max_epi8(avx::_mm512_subs_epi8(h_prev, e_ins_vec), zero);
        avx::_mm512_storeu_si512(h_ptr.add(j * stride) as *mut _, h_curr);
        h_prev = h_curr;
    }

    // Max tracking
    let mut max_scores = h0_vec;
    let mut max_i_vec = avx::_mm512_setzero_si512();
    let mut max_j_vec = avx::_mm512_setzero_si512();

    // Preload per-lane scalars
    let qlen_vec = avx::_mm512_loadu_si512(params.qlen.as_ptr() as *const _);
    let w_vec = avx::_mm512_loadu_si512(params.w.as_ptr() as *const _);
    // beg starts at 0; end starts at qlen
    let beg_vec = avx::_mm512_setzero_si512();
    let mut end_vec = qlen_vec;

    // Per-lane termination
    let mut term_mask_global: u64 = 0; // 1 bit = lane terminated

    for i in 0..tmax {
        // If all active lanes terminated, break
        if term_mask_global.count_ones() as usize >= lanes {
            break;
        }

        let mut f_vec = zero;
        let mut h_diag = avx::_mm512_loadu_si512(h_ptr as *const _);

        // Load target row s1
        let s1 = avx::_mm512_loadu_si512(params.target_soa.as_ptr().add(i * stride) as *const _);

        let i_vec = avx::_mm512_set1_epi8(i as i8);

        // Compute band bounds for this row
        let i_minus_w = avx::_mm512_subs_epi8(i_vec, w_vec);
        let current_beg_vec = avx::_mm512_max_epi8(beg_vec, i_minus_w);
        let i_plus_w = avx::_mm512_adds_epi8(i_vec, w_vec);
        let i_plus_w_plus_1 = avx::_mm512_adds_epi8(i_plus_w, one);
        let mut current_end_vec = avx::_mm512_min_epu8(end_vec, i_plus_w_plus_1);
        current_end_vec = avx::_mm512_min_epu8(current_end_vec, qlen_vec);

        // Determine lanes that are still active at row i
        // Build mask: !terminated && i < tlen
        let tlen_vec = avx::_mm512_loadu_si512(params.tlen.as_ptr() as *const _);
        let i_lt_tlen_mask: u64 = avx::_mm512_cmpgt_epu8_mask(tlen_vec, i_vec);
        let active_mask: u64 = (!term_mask_global) & i_lt_tlen_mask;

        // Track per-row max for zdrop
        let mut row_max_vec = zero;

        for j in 0..qmax {
            let h_top = avx::_mm512_loadu_si512(h_ptr.add(j * stride) as *const _);
            let e_prev = avx::_mm512_loadu_si512(e_ptr.add(j * stride) as *const _);

            let h_diag_curr = h_diag;
            h_diag = h_top;

            // Load query column s2
            let s2 = avx::_mm512_loadu_si512(params.query_soa.as_ptr().add(j * stride) as *const _);

            // Score: match vs mismatch (N handling via OR mask like generic)
            let eq_mask = avx::_mm512_cmpeq_epi8_mask(s1, s2);
            let score_vec = avx::_mm512_mask_blend_epi8(eq_mask, mismatch_vec, match_vec);
            let or_bases = avx::_mm512_or_si512(s1, s2);
            let m_add = avx::_mm512_adds_epi8(h_diag_curr, score_vec);
            // Zero where ambiguous (any base >= 4)
            let ambig_mask = avx::_mm512_cmpgt_epu8_mask(or_bases, avx::_mm512_set1_epi8(3));
            let mut m_vec = avx::_mm512_mask_blend_epi8(ambig_mask, m_add, zero);
            m_vec = avx::_mm512_max_epi8(m_vec, zero);

            // E and F updates (saturating)
            let e_open = avx::_mm512_max_epi8(avx::_mm512_subs_epi8(m_vec, oe_del_vec), zero);
            let e_extend = avx::_mm512_subs_epi8(e_prev, e_del_vec);
            let e_val = avx::_mm512_max_epi8(e_open, e_extend);

            let f_open = avx::_mm512_max_epi8(avx::_mm512_subs_epi8(m_vec, oe_ins_vec), zero);
            let f_extend = avx::_mm512_subs_epi8(f_vec, e_ins_vec);
            f_vec = avx::_mm512_max_epi8(f_open, f_extend);

            // Candidate H
            let mut h_val = avx::_mm512_max_epi8(m_vec, e_val);
            h_val = avx::_mm512_max_epi8(h_val, f_vec);

            // In-band check: (j > current_beg-1) & (j < current_end)
            let j_vec = avx::_mm512_set1_epi8(j as i8);
            let in_band_left =
                avx::_mm512_cmpgt_epi8_mask(j_vec, avx::_mm512_subs_epi8(current_beg_vec, one));
            let in_band_right = avx::_mm512_cmpgt_epi8_mask(current_end_vec, j_vec);
            let in_band_mask = in_band_left & in_band_right;

            // Combined lane mask for valid work at (i,j)
            let combined_mask: u64 = in_band_mask & active_mask;

            // Mask h/e values outside band/active
            let h_masked = avx::_mm512_maskz_mov_epi8(combined_mask, h_val);
            let e_masked = avx::_mm512_maskz_mov_epi8(combined_mask, e_val);

            avx::_mm512_storeu_si512(h_ptr.add(j * stride) as *mut _, h_masked);
            avx::_mm512_storeu_si512(e_ptr.add(j * stride) as *mut _, e_masked);

            // Max tracking
            let gt_mask = avx::_mm512_cmpgt_epi8_mask(h_masked, max_scores);
            max_scores = avx::_mm512_max_epi8(h_masked, max_scores);
            let i_blend =
                avx::_mm512_mask_blend_epi8(gt_mask, max_i_vec, avx::_mm512_set1_epi8(i as i8));
            let j_blend =
                avx::_mm512_mask_blend_epi8(gt_mask, max_j_vec, avx::_mm512_set1_epi8(j as i8));
            max_i_vec = i_blend;
            max_j_vec = j_blend;

            row_max_vec = avx::_mm512_max_epi8(row_max_vec, h_masked);
        }

        if zdrop > 0 {
            // Determine lanes to terminate using zdrop based on row_max vs global max
            let global_gt_row_mask: u64 = avx::_mm512_cmpgt_epi8_mask(max_scores, row_max_vec);
            // Compute (global - row) > zdrop using i16 to avoid wrap; do approximate: if max>row and (max - row) as i16 > zdrop
            // For simplicity and to preserve correctness biases, we conservatively terminate when row_max==0 as in generic fast path
            let row_zero_mask: u64 = avx::_mm512_cmpeq_epi8_mask(row_max_vec, zero);
            // Only consider active lanes
            let term_now = (row_zero_mask | global_gt_row_mask) & active_mask;
            term_mask_global |= term_now;
        }
    }

    // Extract results
    let mut out_scores = vec![];
    let mut max_scores_arr = [0i8; W];
    let mut max_i_arr = [0i8; W];
    let mut max_j_arr = [0i8; W];
    avx::_mm512_storeu_si512(max_scores_arr.as_mut_ptr() as *mut _, max_scores);
    avx::_mm512_storeu_si512(max_i_arr.as_mut_ptr() as *mut _, max_i_vec);
    avx::_mm512_storeu_si512(max_j_arr.as_mut_ptr() as *mut _, max_j_vec);

    for i in 0..lanes {
        out_scores.push(OutScore {
            score: max_scores_arr[i] as i32,
            target_end_pos: max_i_arr[i] as i32,
            query_end_pos: max_j_arr[i] as i32,
            gtarget_end_pos: 0,
            global_score: 0,
            max_offset: 0,
        });
    }

    out_scores
}
